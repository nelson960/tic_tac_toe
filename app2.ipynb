{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73612aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24439db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# config + helpers\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    steps: int = 2000\n",
    "    games_per_step: int = 64\n",
    "\n",
    "    beta: float = 0.02\n",
    "    value_coef: float = 1.0\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # exploration schedule\n",
    "    temp_start: float = 1.5\n",
    "    temp_end: float = 0.3\n",
    "    eps_start: float = 0.10\n",
    "    eps_end: float = 0.02\n",
    "\n",
    "    # logging / eval\n",
    "    eval_every: int = 100\n",
    "    eval_games: int = 200\n",
    "\n",
    "    # printing\n",
    "    print_every: int = 100         # print EVERY step (line-by-line)\n",
    "    show_progress_bar: bool = True  # set True if you still want tqdm bar\n",
    "\n",
    "    # speed toggles\n",
    "    compile_model: bool = False  # torch.compile (PyTorch 2.x), sometimes faster\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, alpha: float = 0.05):\n",
    "        self.alpha = alpha\n",
    "        self.value: Optional[float] = None\n",
    "\n",
    "    def update(self, x: float) -> float:\n",
    "        self.value = x if self.value is None else self.alpha * x + (1 - self.alpha) * self.value\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18401eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# TicTacToe environment (single-game, for eval)\n",
    "# -------------------------\n",
    "\n",
    "WIN_LINES = [\n",
    "    (0, 1, 2), (3, 4, 5), (6, 7, 8),  # rows\n",
    "    (0, 3, 6), (1, 4, 7), (2, 5, 8),  # cols\n",
    "    (0, 4, 8), (2, 4, 6)              # diags\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class TicTacToe:\n",
    "    # board: 0 empty, +1 X, -1 O\n",
    "    board: List[int]\n",
    "    player: int  # +1 X to move, -1 O to move\n",
    "\n",
    "    @staticmethod\n",
    "    def new():\n",
    "        return TicTacToe(board=[0] * 9, player=+1)\n",
    "\n",
    "    def legal_moves(self) -> List[int]:\n",
    "        return [i for i, v in enumerate(self.board) if v == 0]\n",
    "\n",
    "    def is_terminal(self) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Returns (done, winner):\n",
    "          done: bool\n",
    "          winner: +1 if X wins, -1 if O wins, 0 if draw/ongoing\n",
    "        \"\"\"\n",
    "        for a, b, c in WIN_LINES:\n",
    "            s = self.board[a] + self.board[b] + self.board[c]\n",
    "            if s == 3:\n",
    "                return True, +1\n",
    "            if s == -3:\n",
    "                return True, -1\n",
    "\n",
    "        if all(v != 0 for v in self.board):\n",
    "            return True, 0  # draw\n",
    "\n",
    "        return False, 0\n",
    "\n",
    "    def step(self, action: int) -> Tuple[bool, int]:\n",
    "        if self.board[action] != 0:\n",
    "            raise ValueError(f\"Illegal move: {action}\")\n",
    "        self.board[action] = self.player\n",
    "        done, winner = self.is_terminal()\n",
    "        if not done:\n",
    "            self.player *= -1\n",
    "        return done, winner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb733727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# encoding + model\n",
    "# -------------------------\n",
    "\n",
    "def board_to_tokens_perspective(board: List[int], player: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Perspective encoding:\n",
    "      board values are in {0, +1, -1}\n",
    "      multiply by player => {0, +1 (self), -1 (opponent)}\n",
    "      map to tokens:\n",
    "        0 -> 0 (empty)\n",
    "        +1 -> 1 (self)\n",
    "        -1 -> 2 (opponent)\n",
    "    \"\"\"\n",
    "    toks = []\n",
    "    for v in board:\n",
    "        pv = v * player\n",
    "        if pv == 0:\n",
    "            toks.append(0)\n",
    "        elif pv == +1:\n",
    "            toks.append(1)\n",
    "        else:\n",
    "            toks.append(2)\n",
    "    return torch.tensor(toks, dtype=torch.long)\n",
    "\n",
    "def masked_softmax(logits: torch.Tensor, legal_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: [B, 9]\n",
    "    legal_mask: [B, 9] bool\n",
    "    \"\"\"\n",
    "    masked = logits.masked_fill(~legal_mask, -1e9)\n",
    "    return F.softmax(masked, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e54e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTTTTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        ff_mult: int = 4,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(3, d_model)  # empty/self/opp\n",
    "        self.pos_emb = nn.Embedding(9, d_model)    # 9 squares\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ff_mult,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.policy_head = nn.Linear(d_model, 9)\n",
    "        self.value_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        tokens: [B, 9] long in {0,1,2}\n",
    "        returns:\n",
    "          logits: [B, 9]\n",
    "          value:  [B]\n",
    "        \"\"\"\n",
    "        B = tokens.size(0)\n",
    "        pos = torch.arange(9, device=tokens.device).unsqueeze(0).expand(B, 9)  # [B,9]\n",
    "\n",
    "        x = self.token_emb(tokens) + self.pos_emb(pos)  # [B,9,d]\n",
    "        x = self.drop(x)\n",
    "        x = self.encoder(x)                              # [B,9,d]\n",
    "\n",
    "        pooled = x.mean(dim=1)                           # [B,d]\n",
    "        logits = self.policy_head(pooled)                # [B,9]\n",
    "        value = torch.tanh(self.value_head(pooled)).squeeze(-1)  # [B]\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4356e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# FAST vectorized self-play (the big speedup)\n",
    "# -------------------------\n",
    "\n",
    "WIN_LINES_T = torch.tensor([\n",
    "    [0,1,2],[3,4,5],[6,7,8],\n",
    "    [0,3,6],[1,4,7],[2,5,8],\n",
    "    [0,4,8],[2,4,6]\n",
    "], dtype=torch.long)\n",
    "\n",
    "def board_to_tokens_perspective_batch(board: torch.Tensor, player: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    board:  (B,9) in {-1,0,+1} (integer)\n",
    "    player: (B,)  in {-1,+1}\n",
    "    returns tokens: (B,9) long with mapping empty=0, self=1, opp=2\n",
    "    \"\"\"\n",
    "    rel = board * player[:, None]  # self=+1, opp=-1\n",
    "    tokens = torch.zeros_like(rel, dtype=torch.long)\n",
    "    tokens[rel == 1] = 1\n",
    "    tokens[rel == -1] = 2\n",
    "    return tokens\n",
    "\n",
    "def check_winner_batch(board: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    board: (B,9) in {-1,0,+1}\n",
    "    returns winner: (B,) in {-1,0,+1}\n",
    "    \"\"\"\n",
    "    lines = board[:, WIN_LINES_T.to(board.device)]  # (B,8,3)\n",
    "    sums = lines.sum(dim=2)                         # (B,8)\n",
    "    winner = torch.zeros((board.size(0),), dtype=torch.int16, device=board.device)\n",
    "    winner[(sums == 3).any(dim=1)] = 1\n",
    "    winner[(sums == -3).any(dim=1)] = -1\n",
    "    return winner\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_self_play_batch_vectorized(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    games: int,\n",
    "    temperature: float,\n",
    "    epsilon: float,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    was_training = model.training\n",
    "    \"\"\"\n",
    "    Plays 'games' self-play games in parallel (vectorized) and returns one flat batch.\n",
    "    Much faster than looping games one-by-one.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    B = games\n",
    "    board = torch.zeros((B, 9), dtype=torch.int16, device=device)   # {-1,0,+1}\n",
    "    player = torch.ones((B,), dtype=torch.int16, device=device)     # start with +1\n",
    "    done = torch.zeros((B,), dtype=torch.bool, device=device)\n",
    "    winners = torch.zeros((B,), dtype=torch.int16, device=device)\n",
    "    lengths = torch.zeros((B,), dtype=torch.int64, device=device)\n",
    "\n",
    "    states_list: List[torch.Tensor] = []\n",
    "    actions_list: List[torch.Tensor] = []\n",
    "    masks_list: List[torch.Tensor] = []\n",
    "    players_list: List[torch.Tensor] = []\n",
    "    game_ids_list: List[torch.Tensor] = []\n",
    "\n",
    "    for _ply in range(9):\n",
    "        active = ~done\n",
    "        if not active.any():\n",
    "            break\n",
    "\n",
    "        idx = torch.where(active)[0]    # active game indices\n",
    "        b = board[idx]                  # (A,9)\n",
    "        p = player[idx]                 # (A,)\n",
    "        legal_mask = (b == 0)           # (A,9) bool\n",
    "\n",
    "        tokens = board_to_tokens_perspective_batch(b, p)  # (A,9) long\n",
    "        logits, _ = model(tokens)                         # (A,9)\n",
    "\n",
    "        A = idx.numel()\n",
    "        actions = torch.empty((A,), dtype=torch.long, device=device)\n",
    "\n",
    "        # epsilon-greedy: some random moves\n",
    "        use_rand = (torch.rand((A,), device=device) < epsilon)\n",
    "\n",
    "        if use_rand.any():\n",
    "            w = legal_mask[use_rand].float()\n",
    "            actions[use_rand] = torch.multinomial(w, 1).squeeze(1)\n",
    "\n",
    "        if (~use_rand).any():\n",
    "            lm = legal_mask[~use_rand]\n",
    "            lg = logits[~use_rand]\n",
    "\n",
    "            if temperature <= 0:\n",
    "                masked = lg.masked_fill(~lm, -1e9)\n",
    "                actions[~use_rand] = masked.argmax(dim=1)\n",
    "            else:\n",
    "                probs = masked_softmax(lg / temperature, lm)\n",
    "                actions[~use_rand] = torch.multinomial(probs, 1).squeeze(1)\n",
    "\n",
    "        # record transitions (state before applying action)\n",
    "        states_list.append(tokens)         # (A,9)\n",
    "        actions_list.append(actions)       # (A,)\n",
    "        masks_list.append(legal_mask)      # (A,9)\n",
    "        players_list.append(p)             # (A,)\n",
    "        game_ids_list.append(idx)          # (A,)\n",
    "\n",
    "        # apply moves\n",
    "        board[idx, actions] = p\n",
    "        lengths[idx] += 1\n",
    "\n",
    "        # terminal check\n",
    "        w = check_winner_batch(board)                 # (B,)\n",
    "        full = (board != 0).all(dim=1)\n",
    "        newly_done = (~done) & ((w != 0) | full)\n",
    "\n",
    "        winners[newly_done] = w[newly_done]           # draw remains 0\n",
    "        done[newly_done] = True\n",
    "\n",
    "        # switch player for still-active games\n",
    "        player[~done] = -player[~done]\n",
    "\n",
    "    # flatten transitions\n",
    "    states = torch.cat(states_list, dim=0)     # (N,9) long on device\n",
    "    actions = torch.cat(actions_list, dim=0)   # (N,) long on device\n",
    "    masks = torch.cat(masks_list, dim=0)       # (N,9) bool on device\n",
    "    pl = torch.cat(players_list, dim=0)        # (N,) int16 on device\n",
    "    gids = torch.cat(game_ids_list, dim=0)     # (N,) long on device\n",
    "\n",
    "    # z per transition = winner_of_game * player_at_state\n",
    "    z = winners[gids].to(torch.float32) * pl.to(torch.float32)\n",
    "    \n",
    "\n",
    "    if was_training:\n",
    "          model.train()\n",
    "          \n",
    "    return {\n",
    "\t\t\t\t\"states\": states,\n",
    "\t\t\t\t\"actions\": actions,\n",
    "\t\t\t\t\"masks\": masks,\n",
    "\t\t\t\t\"z\": z,\n",
    "\t\t\t\t\"winners\": winners.to(torch.int64).cpu(),\n",
    "\t\t\t\t\"lengths\": lengths.cpu(),\n",
    "\t\t\t}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# loss + eval\n",
    "# -------------------------\n",
    "\n",
    "def compute_loss(\n",
    "    logits: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    legal_mask: torch.Tensor,\n",
    "    z: torch.Tensor,\n",
    "    beta: float = 0.01,\n",
    "    value_coef: float = 1.0,\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    probs = masked_softmax(logits, legal_mask)  # [N,9]\n",
    "\n",
    "    # log pi(a|s)\n",
    "    a = actions.unsqueeze(1)                    # [N,1]\n",
    "    pa = probs.gather(1, a).squeeze(1)          # [N]\n",
    "    logp = torch.log(pa.clamp_min(1e-12))       # stable\n",
    "\n",
    "    # entropy\n",
    "    entropy = -(probs * torch.log(probs.clamp_min(1e-12))).sum(dim=1)  # [N]\n",
    "\n",
    "    policy_loss = -(z * logp).mean()\n",
    "    value_loss = F.mse_loss(values, z)\n",
    "    entropy_bonus = entropy.mean()\n",
    "\n",
    "    total = policy_loss + value_coef * value_loss - beta * entropy_bonus\n",
    "\n",
    "    stats = {\n",
    "        \"loss\": float(total.item()),\n",
    "        \"policy_loss\": float(policy_loss.item()),\n",
    "        \"value_loss\": float(value_loss.item()),\n",
    "        \"entropy\": float(entropy_bonus.item()),\n",
    "    }\n",
    "    return total, stats\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_vs_random(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    games: int = 200,\n",
    ") -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    wins = draws = losses = 0\n",
    "\n",
    "    for g in range(games):\n",
    "        env = TicTacToe.new()\n",
    "        model_side = +1 if (g % 2 == 0) else -1\n",
    "\n",
    "        while True:\n",
    "            done, winner = env.is_terminal()\n",
    "            if done:\n",
    "                if winner == 0:\n",
    "                    draws += 1\n",
    "                elif winner == model_side:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                break\n",
    "\n",
    "            legal = env.legal_moves()\n",
    "\n",
    "            if env.player == model_side:\n",
    "                legal_mask = torch.zeros(9, dtype=torch.bool)\n",
    "                legal_mask[legal] = True\n",
    "                tokens = board_to_tokens_perspective(env.board, env.player).to(device).unsqueeze(0)  # [1,9]\n",
    "                logits, _ = model(tokens)\n",
    "                logits = logits.squeeze(0)\n",
    "                masked = logits.masked_fill(~legal_mask.to(device), -1e9)\n",
    "                action = int(masked.argmax().item())\n",
    "            else:\n",
    "                action = random.choice(legal)\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "    total = wins + draws + losses\n",
    "    return wins / total, draws / total, losses / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# main training loop\n",
    "# -------------------------\n",
    "\n",
    "def pick_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = TrainConfig()\n",
    "    device = pick_device()\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    model = TinyTTTTransformer(\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        ff_mult=4,\n",
    "        dropout=0.0\n",
    "    ).to(device)\n",
    "\n",
    "    if cfg.compile_model and hasattr(torch, \"compile\"):\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    ema_loss = EMA(alpha=0.05)\n",
    "    ema_pi = EMA(alpha=0.05)\n",
    "    ema_v = EMA(alpha=0.05)\n",
    "\n",
    "    if cfg.show_progress_bar:\n",
    "        iterator = trange(1, cfg.steps + 1, desc=\"training\", unit=\"step\")\n",
    "        log = tqdm.write\n",
    "    else:\n",
    "        iterator = range(1, cfg.steps + 1)\n",
    "        log = print\n",
    "\n",
    "    for step in iterator:\n",
    "        model.train()\n",
    "        step_t0 = time.perf_counter()\n",
    "\n",
    "        # linear schedule\n",
    "        t = step / cfg.steps\n",
    "        temperature = cfg.temp_start + t * (cfg.temp_end - cfg.temp_start)\n",
    "        epsilon = cfg.eps_start + t * (cfg.eps_end - cfg.eps_start)\n",
    "\n",
    "        # fast vectorized self-play\n",
    "        sp_t0 = time.perf_counter()\n",
    "        batch = collect_self_play_batch_vectorized(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            games=cfg.games_per_step,\n",
    "            temperature=temperature,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        sp_sec = time.perf_counter() - sp_t0\n",
    "\n",
    "        logits, values = model(batch[\"states\"])\n",
    "        loss, stats = compute_loss(\n",
    "            logits=logits,\n",
    "            values=values,\n",
    "            actions=batch[\"actions\"],\n",
    "            legal_mask=batch[\"masks\"],\n",
    "            z=batch[\"z\"],\n",
    "            beta=cfg.beta,\n",
    "            value_coef=cfg.value_coef,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        winners = batch[\"winners\"]\n",
    "        x_wins = int((winners == +1).sum().item())\n",
    "        o_wins = int((winners == -1).sum().item())\n",
    "        draws = int((winners == 0).sum().item())\n",
    "        avg_len = float(batch[\"lengths\"].float().mean().item())\n",
    "\n",
    "        s_loss = ema_loss.update(stats[\"loss\"])\n",
    "        s_pi = ema_pi.update(stats[\"policy_loss\"])\n",
    "        s_v = ema_v.update(stats[\"value_loss\"])\n",
    "\n",
    "        step_sec = time.perf_counter() - step_t0\n",
    "\n",
    "        # print every step line-by-line (your requested format)\n",
    "        if (step % cfg.print_every) == 0:\n",
    "            log(\n",
    "                f\"[step {step:4d}] \"\n",
    "                f\"loss {s_loss:.4f} | pi {s_pi:.4f} | v {s_v:.4f} | H {stats['entropy']:.3f} | \"\n",
    "                f\"temp {temperature:.2f} | eps {epsilon:.2f} | avgT {avg_len:.2f} | \"\n",
    "                f\"X {x_wins:3d} O {o_wins:3d} D {draws:3d} | \"\n",
    "                f\"sp_s {sp_sec:.2f} | step_s {step_sec:.2f}\"\n",
    "            )\n",
    "\n",
    "        # evaluation\n",
    "        if (step % cfg.eval_every) == 0:\n",
    "            win, draw, lose = eval_vs_random(model, device, games=cfg.eval_games)\n",
    "            log(\n",
    "                f\"  eval vs random ({cfg.eval_games}): \"\n",
    "                f\"W {win:.2%} D {draw:.2%} L {lose:.2%}\"\n",
    "            )\n",
    "\n",
    "    torch.save(model.state_dict(), \"ttt_transformer.pt\")\n",
    "    print(\"Saved model to ttt_transformer.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
