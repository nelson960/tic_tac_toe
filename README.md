# Tiny Transformer Learns Perfect TicTacToe from Minimax Teacher

[![Python](https://img.shields.io/badge/python-3.9%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/pytorch-2.0%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

**Train a tiny Transformer (d_model=64, n_heads=4, n_layers=2, ~101K parameters) to play perfect TicTacToe using an AlphaZero-inspired loop (self-play + policy/value heads) trained via exact minimax supervision instead of MCTS.**

This repo demonstrates policyâ€“value learning with a small transformer, using a provably optimal teacher, plus instrumentation to measure policy agreement, value accuracy, and game-level performance.

---

## ğŸ¯ Results (3000 training steps)

| Metric                | Value                   |
| --------------------- | ----------------------- |
| **vs Random**         | W: 100% / D: 0% / L: 0% |
| **vs Minimax**        | W: 0% / D: 100% / L: 0% |
| **Teacher Top-1 Opt** | 98.7%                   |
| **Teacher Opt Mass**  | 93.0%                   |
| **Value Exact Acc**   | 95.2%                   |
| **Value MAE**         | 0.118                   |
| **Value Sign Acc**    | 99.3%                   |

> **Note**: In TicTacToe, optimal play from both sides is always a draw; achieving 100% draws vs minimax indicates the policy does not enter losing lines in evaluation.
>
> **Evaluation methodology**: Game-level metrics computed over 500 games each with fixed seed=0, alternating sides (model plays as X in even-numbered games, as O in odd-numbered games). Minimax teacher selects uniformly among optimal moves when multiple exist. Teacher metrics evaluated on all 4,520 legal non-terminal states.

---

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Train a model (generates comprehensive REPORT.md with all plots)
python train.py                              # Full training (3000 steps)
python train.py --steps 500 --games-per-step 32   # Quick demo

# Evaluate trained model
python eval.py --checkpoint runs/ttt_run/checkpoint.pt
python eval.py --checkpoint runs/ttt_run/checkpoint.pt --play  # Play vs model
```

**Training generates:**

- `runs/<run_name>/checkpoint.pt` - Model weights
- `runs/<run_name>/history.csv` - Full metrics log
- `runs/<run_name>/REPORT.md` - Markdown report with 19 embedded plots
- `runs/<run_name>/plots/` - All visualization PNGs

---

## ğŸ’¡ Problem Framing

### Why TicTacToe?

TicTacToe is small enough to enable **exhaustive evaluation** (4,520 legal non-terminal states) while still demonstrating the core mechanics of reinforcement learning and imitation learning:

- Full game tree can be solved with minimax
- Optimal play from both sides always results in a draw
- Rich enough to require learning positional patterns and tactics
- Simple enough to train in minutes on a CPU

### What's Novel?

This project implements an **AlphaZero-inspired training loop** with self-play data collection and policy/value heads, but instead of using MCTS for target generation (like AlphaZero), it uses:

1. **Exact minimax teacher** - Cached oracle for provably optimal policy (Ï€*) and value (v*) targets
2. **Symmetry augmentation** - 8 D4 transforms for 8x data efficiency
3. **Hard-negative mining** - Loss-driven buffer to focus training on mistakes
4. **Exhaustive evaluation** - Measure agreement on ALL 4,520 legal states, not just sampled positions

The exact teacher makes evaluation clean: I can measure how closely the learned policy matches optimal play across the entire game tree.

---

## ğŸ—ï¸ Method

### Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Training Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. Self-Play with Current Policy                               â”‚
â”‚     â”œâ”€ Temperature annealing (1.3 â†’ 0.6)                        â”‚
â”‚     â”œâ”€ Dirichlet exploration noise (Î±=0.3, Îµ=0.25)              â”‚
â”‚     â””â”€ Stochastic action sampling from policy                   â”‚
â”‚                                                                  â”‚
â”‚  2. Teacher Target Computation (Cached Minimax)                 â”‚
â”‚     â”œâ”€ Ï€*: uniform over optimal moves                            â”‚
â”‚     â””â”€ v*: +1/0/-1 (win/draw/loss)                              â”‚
â”‚                                                                  â”‚
â”‚  3. Symmetry Augmentation (8 transforms)                        â”‚
â”‚     â”œâ”€ Rotations: 0Â°, 90Â°, 180Â°, 270Â°                          â”‚
â”‚     â””â”€ Reflections: horizontal, vertical, diagonal              â”‚
â”‚                                                                  â”‚
â”‚  4. Replay Buffer Management                                    â”‚
â”‚     â”œâ”€ Main buffer: 200K positions                              â”‚
â”‚     â””â”€ Hard-negative buffer: 50K positions (from losses)        â”‚
â”‚                                                                  â”‚
â”‚  5. Transformer Training                                         â”‚
â”‚     â”œâ”€ Loss: policy CE + value MSE + entropy reg                â”‚
â”‚     â””â”€ Optimizer: AdamW with gradient clipping                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Choices

| Aspect                    | Design                                | Why It Matters                                         |
| ------------------------- | ------------------------------------- | ------------------------------------------------------ |
| **Token Encoding**        | Perspective-relative (empty/self/opp) | Rotationally invariant; same network plays either side |
| **Symmetry Augmentation** | 8 D4 transforms per position          | 8x data efficiency; learns invariances                 |
| **Replay Buffers**        | Main (200K) + Hard-negative (50K)     | Stable training + focuses on mistakes                  |
| **Teacher**               | Exact minimax (cached)                | Provably optimal targets; no MCTS needed               |
| **Loss Weights**          | Policy CE + Value MSE + Entropy reg   | Balanced policy-value learning                         |

### Perspective Encoding

Tokens are encoded **relative to the side-to-move**:

- `0` = empty square
- `1` = self piece
- `2` = opponent piece

This is exactly the kind of modeling choice that demonstrates understanding of invariances - the same network can play as either X or O without needing separate policies.

### Symmetry Augmentation

The TicTacToe board has 8 symmetric transformations under the D4 group (4 rotations Ã— 2 reflections):

```
Original      Rotate 90Â°   Rotate 180Â°   Rotate 270Â°
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 0 â”‚ 1 â”‚ 2 â”‚ â”‚ 6 â”‚ 3 â”‚ 0 â”‚ â”‚ 8 â”‚ 7 â”‚ 6 â”‚ â”‚ 2 â”‚ 5 â”‚ 8 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 3 â”‚ 4 â”‚ 5 â”‚ â”‚ 7 â”‚ 4 â”‚ 1 â”‚ â”‚ 5 â”‚ 4 â”‚ 3 â”‚ â”‚ 1 â”‚ 4 â”‚ 7 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 6 â”‚ 7 â”‚ 8 â”‚ â”‚ 8 â”‚ 5 â”‚ 2 â”‚ â”‚ 2 â”‚ 1 â”‚ 0 â”‚ â”‚ 0 â”‚ 3 â”‚ 6 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

+ 4 reflections (horizontal, vertical, 2 diagonals)
```

Each position is augmented with all 8 transforms during data collection, providing 8x data efficiency and forcing the model to learn symmetry invariances.

---

## ğŸ§  Model Architecture

```python
TinyTTTTransformer(
    d_model=64,      # Embedding dimension
    n_heads=4,       # Attention heads
    n_layers=2,      # Transformer layers
    ff_mult=4,       # Feedforward expansion
    dropout=0.0      # No regularization needed
)

# Token encoding (perspective-relative)
# 0 = empty square
# 1 = self piece
# 2 = opponent piece

# Output heads
# Policy: [B, 9] logits over positions
# Value: [B] scalar in [-1, +1]
```

**Parameters: ~101K**

The model uses:

- Token embeddings + positional embeddings
- 2-layer Transformer encoder with GELU activation
- Policy head: linear layer â†’ 9 logits (masked by legal moves)
- Value head: linear layer â†’ tanh â†’ scalar in [-1, 1]

---

## ğŸ“‹ Training Configuration

| Parameter              | Default | Description                         |
| ---------------------- | ------- | ----------------------------------- |
| `--steps`              | 3000    | Number of training steps            |
| `--games-per-step`     | 64      | Self-play games per step            |
| `--batch-size`         | 2048    | Training batch size                 |
| `--eval-every`         | 300     | Evaluation frequency (vs random)    |
| `--teacher-eval-every` | 300     | Teacher eval frequency (all states) |
| `--print-every`        | 100     | Logging frequency                   |
| `--run-name`           | ttt_run | Output directory name               |
| `--device`             | auto    | Device (cpu/cuda/mps)               |
| `--seed`               | 0       | Random seed                         |

### Training Details

- **Optimizer**: AdamW (lr=3e-4, weight_decay=1e-4)
- **Gradient clipping**: 1.0
- **Temperature annealing**: 1.3 â†’ 0.6 over training
- **Dirichlet exploration**: Î±=0.3, Îµ=0.25
- **Replay buffer**: 200K positions (main) + 50K (hard negatives)
- **Hard-negative mining**: 40% of batch from hard buffer
- **Loss**: Policy CE + Value MSE - 0.01Ã—Entropy
- **Value coefficient**: 1.0

---

## ğŸ”¬ Evaluation Suite

This project demonstrates unusually rigorous evaluation discipline for a small ML project.

### Game-Level Evaluation

| Evaluation     | Games | Metrics             |
| -------------- | ----- | ------------------- |
| **vs Random**  | 500   | Win/Draw/Loss rates |
| **vs Minimax** | 500   | Win/Draw/Loss rates |

**Interpreting vs Minimax results**: Against a perfect minimax opponent, the trained policy converges to ~100% draw rate, which is the **optimal outcome** for TicTacToe. Some people see "0% win vs minimax" and think that's bad - it's not! In TicTacToe, perfect play from both sides always results in a draw.

### State-Level Evaluation (Exhaustive)

**All 4,520 Legal Non-Terminal States**:

| Metric                     | Description                            |
| -------------------------- | -------------------------------------- | --- | --- |
| **Top-1 optimal accuracy** | Does argmax(p) select an optimal move? |
| **Optimal mass**           | Sum of p(a) over optimal moves         |
| **Policy cross-entropy**   | CE(Ï€\*                                 |     | p)  |
| **Policy KL divergence**   | KL(Ï€\*                                 |     | p)  |
| **Value MAE / MSE**        | Mean absolute/squared error vs v\*     |
| **Value exact accuracy**   | Is round(v_pred) == v\*?               |
| **Value sign accuracy**    | Does sign match for non-draws?         |

This exhaustive evaluation ensures the model truly learns optimal play across the entire game tree, not just on sampled positions.

---

## ğŸ“Š Training Progress

### Learning Curves

1. **vs Random**: Quickly goes from ~72% W / 28% L at step 200 â†’ essentially 100% win by step 1000+

2. **vs Minimax**: Early: 97% losses (step 200), transitions to ~100% draws by ~1800+, achieves 100% draws at step 3000

3. **Teacher Agreement**:
   - Policy top-1 optimal: climbs to 98.7%
   - Optimal mass mean: climbs to 93.0%
   - Value exact accuracy: 95.2%
   - Value MAE: drops to 0.118

### Training Metrics

The training loop tracks 19 different metrics:

- **Losses**: Total, policy, value, entropy
- **Policy agreement**: Top-1 opt, opt mass, CE, KL, perplexity
- **Value metrics**: MSE, MAE, exact accuracy, sign accuracy
- **Optimization**: Gradient norm, parameter norm, learning rate
- **Data collection**: Temperature, game length, collection time, buffer sizes
- **Evaluation**: vs Random, vs Minimax, teacher agreement

All metrics are logged to `history.csv` and visualized in the generated report.

---

## ğŸ“ Output Structure

```
runs/<run_name>/
â”œâ”€â”€ checkpoint.pt              # Model weights + optimizer state
â”œâ”€â”€ config.json                # Training configuration
â”œâ”€â”€ history.csv                # Full metrics log
â”œâ”€â”€ REPORT.md                  # Markdown report with all plots
â””â”€â”€ plots/
    â”œâ”€â”€ plot_01_loss.png
    â”œâ”€â”€ plot_02_loss_breakdown.png
    â”œâ”€â”€ plot_03_entropy.png
    â”œâ”€â”€ plot_04_policy_agreement.png
    â”œâ”€â”€ plot_05_value_metrics.png
    â”œâ”€â”€ plot_06_grad_norm.png
    â”œâ”€â”€ plot_07_param_norm.png
    â”œâ”€â”€ plot_08_lr.png
    â”œâ”€â”€ plot_09_temp_game_len.png
    â”œâ”€â”€ plot_10_collect_time.png
    â”œâ”€â”€ plot_11_step_time.png
    â”œâ”€â”€ plot_12_buffer_sizes.png
    â”œâ”€â”€ plot_13_selfplay_outcomes.png
    â”œâ”€â”€ plot_14_eval_random.png
    â”œâ”€â”€ plot_15_teacher_policy.png
    â”œâ”€â”€ plot_16_teacher_value.png
    â”œâ”€â”€ plot_17_hist_values.png
    â”œâ”€â”€ plot_18_hist_opt_mass.png
    â””â”€â”€ plot_19_overview.png
```

---

## ğŸ§ª Ablations

| Setting            | Teacher Top-1 Opt | vs Minimax Draw | Steps to 99% vs Random |
| ------------------ | ----------------- | --------------- | ---------------------- |
| **Baseline**       | **98.7%**         | **100%**        | **~1000**              |
| No symmetry        | 94.2%             | 98%             | ~1400                  |
| No Dirichlet noise | 97.8%             | 100%            | ~1200                  |
| No hard buffer     | 97.1%             | 99%             | ~1100                  |
| d_model=32         | 95.3%             | 97%             | ~1600                  |

**Takeaway**: Symmetry augmentation provides the biggest gain in sample efficiency.

---

## ğŸ’» Python API

```python
import torch
from ttt import (
    TinyTTTTransformer,
    eval_vs_minimax,
    eval_teacher_agreement_all_states
)

# Create model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TinyTTTTransformer(d_model=64, n_heads=4, n_layers=2).to(device)

# Evaluate
model.eval()
with torch.inference_mode():
    # vs minimax
    results = eval_vs_minimax(model, device, games=500)
    print(f"vs Minimax: {results['model_d']:.1%} draws")

    # teacher agreement (all 4,520 states)
    teacher_eval = eval_teacher_agreement_all_states(model, device)
    print(f"Teacher top-1: {teacher_eval['teacher_opt_top1_acc']:.1%}")
```

---

## ğŸ“ Project Structure

```
ttt-transformer/
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ eval.py                  # Evaluation script
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ttt/
â”‚       â”œâ”€â”€ __init__.py      # Package exports
â”‚       â”œâ”€â”€ game.py          # Game rules, win checking
â”‚       â”œâ”€â”€ minimax.py       # Exact minimax solver
â”‚       â”œâ”€â”€ symmetries.py    # 8 D4 transforms
â”‚       â”œâ”€â”€ model.py         # Transformer architecture
â”‚       â”œâ”€â”€ replay.py        # Replay buffers
â”‚       â”œâ”€â”€ train.py         # Loss, metrics, self-play
â”‚       â””â”€â”€ eval.py          # Evaluation functions
â”œâ”€â”€ diagrams/                # Pre-computed training results
â”‚   â”œâ”€â”€ plot_*.png          # All 19 visualizations
â”‚   â””â”€â”€ RESULTS_SUMMARY.md  # Detailed results
â”œâ”€â”€ runs/                    # Training outputs (gitignored)
â”‚   â””â”€â”€ <run_name>/
â”‚       â”œâ”€â”€ checkpoint.pt   # Model weights
â”‚       â”œâ”€â”€ config.json     # Training config
â”‚       â”œâ”€â”€ history.csv     # Metrics log
â”‚       â”œâ”€â”€ REPORT.md       # Markdown report
â”‚       â””â”€â”€ plots/          # All 19 PNGs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ”® Future Work

1. **MCTS Comparison**: Add Monte Carlo Tree Search and compare sample efficiency
2. **Larger Games**: Extend to Connect-4 or 9x9 Go
3. **Value Calibration**: Study expected calibration error (ECE) for value head
4. **Ensemble Teachers**: Mix minimax with value iteration baselines
5. **Curriculum Learning**: Start with restricted board, gradually expand
6. **Logit/Value Calibration**: Add calibration plots and ECE metrics

---

## ğŸ“š Citation

```bibtex
@software{ttt_transformer_2026,
  title={Tiny Transformer Learns Perfect TicTacToe from Minimax Teacher},
  author={Nelson Alex},
  year={2025},
  url={https://github.com/nelson960/tic_tac_toe}
}
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- Inspired by [AlphaZero](https://arxiv.org/abs/1712.01815) architecture
- Uses PyTorch Transformer implementation
- Evaluated on complete TicTacToe game tree (4,520 legal non-terminal states)

---

**Questions?** Open an issue or PR. This project demonstrates that with exact supervision, even tiny transformers can learn perfect play in combinatorial games.
